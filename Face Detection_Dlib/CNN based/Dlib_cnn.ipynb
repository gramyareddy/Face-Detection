{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dlib based on CNN\n",
    "\n",
    "Detects faces at odd angles.\n",
    "\n",
    "Architecture of CNN used in Dlib\n",
    "================================\n",
    "\n",
    " template <template <int,template<typename>class,int,typename> class block, int N, template<typename>class BN, typename SUBNET>\n",
    "    using residual = add_prev1<block<N,BN,1,tag1<SUBNET>>>;\n",
    "\n",
    "    template <template <int,template<typename>class,int,typename> class block, int N, template<typename>class BN, typename SUBNET>\n",
    "    using residual_down = add_prev2<avg_pool<2,2,2,2,skip1<tag2<block<N,BN,2,tag1<SUBNET>>>>>>;\n",
    "\n",
    "    template <int N, template <typename> class BN, int stride, typename SUBNET> \n",
    "    using block  = BN<con<N,3,3,1,1,relu<BN<con<N,3,3,stride,stride,SUBNET>>>>>;\n",
    "\n",
    "    template <int N, typename SUBNET> using ares      = relu<residual<block,N,affine,SUBNET>>;\n",
    "    template <int N, typename SUBNET> using ares_down = relu<residual_down<block,N,affine,SUBNET>>;\n",
    "\n",
    "    template <typename SUBNET> using alevel0 = ares_down<256,SUBNET>;\n",
    "    template <typename SUBNET> using alevel1 = ares<256,ares<256,ares_down<256,SUBNET>>>;\n",
    "    template <typename SUBNET> using alevel2 = ares<128,ares<128,ares_down<128,SUBNET>>>;\n",
    "    template <typename SUBNET> using alevel3 = ares<64,ares<64,ares<64,ares_down<64,SUBNET>>>>;\n",
    "    template <typename SUBNET> using alevel4 = ares<32,ares<32,ares<32,SUBNET>>>;\n",
    "\n",
    "    using anet_type = loss_metric<fc_no_bias<128,avg_pool_everything<\n",
    "                                alevel0<\n",
    "                                alevel1<\n",
    "                                alevel2<\n",
    "                                alevel3<\n",
    "                                alevel4<\n",
    "                                max_pool<3,3,2,2,relu<affine<con<32,7,7,2,2,\n",
    "                                input_rgb_image_sized<150>\n",
    "                                >>>>>>>>>>>>;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "print(\"imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    " ‘dlib’ in python uses these weights and detect images,but dlib doesn’t provide this detector as in-built. We must download and provide to ‘dlib’ classes for extracting face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "detector = dlib.cnn_face_detection_model_v1(\"C:\\\\MyLearnings\\\\DeepLearning\\\\Facial Projects\\\\Face Detection\\\\dlib\\\\hog\\\\mmod_human_face_detector.dat\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "flag = 0\n",
    "import time\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "\n",
    "        x1 = rect.rect.left() *4\n",
    "        y1 = rect.rect.top() *4\n",
    "        x2 = rect.rect.right() *4\n",
    "        y2 = rect.rect.bottom() *4\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "\n",
    "\n",
    "    # Display the video output\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "\n",
    "    # Quit video by typing Q\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Comments \n",
    "\n",
    "Sloww, doesnt detect unless complete face in fov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
